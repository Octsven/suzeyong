## Education

University of Electronic Science and Technology of China (UESTC)

Chengdu, Sichuan, China

-GPA: 3.89/4.0

-College: YingCai Honors College of UESTC

-Major: Mathematics and Physics Basic Science

## Introduction
Self-Introduction: I am interested in understanding how we can reliably quantify uncertainty and robustness in modern, complex data analysis procedure. In particular, I am interested in Bayesian inference and graphical models-with an emphasis on scalable, nonparametric, and unsupervised learning. In addition, the primary challenge that has been driving my research is the need for designing highperformance, extremely effective algorithms under severe resource constraints. And my current focus includes modelling irregular time series and tensor decomposition applied in model compression.

Sikll: Python, Tensorflow, Pytorch, Keras

## Research Experience
```markdown
Statistical Machine Intelligence Learning Laboratory (SMILE Lab), UESTC

Sept.2018 - Sept.2019

-Advisor: ZenglinXu
-Focus: Tensor Decomposition; Multi-Task
-Project Description: We introduced novel tensor network structures(Projected entangled pairstate (PEPS)), into Multi-Task models and called Concatenated Tensor Networks for Multi-Task Learning (CT-MTL). Compared to other types of tensor networks, such as Tensor Train (TT) and Tucker decomposition, CT-MTL can avoid correlation decay of the shared cores due to their strong representation power underneath their inherent compact architecture.
```
```markdown
Statistical Machine Intelligence Learning Laboratory (SMILE Lab), UESTC

Sept.2019 - Present

-Advisor: ZenglinXu
-Focus: Temporal Point Process; Time Series
-Project Description: We proposed a novel temporal point process model, i.e. the Cumulative-Density function based Temporal Point Process, to learn the underlying representation by accurately estimating cumulative density function via the monotonic neural network.
```
```markdown
Statistical Machine Intelligence Learning Laboratory (SMILE Lab), UESTC

Jan.2020 - Present

-Advisor: ZenglinXu
-Focus: Model Optimization; Initialization Method; Tensor Neural Network
-Project Description: General initialization methods are not suitable for tensor networks. We propose a unified initialization method suitable for tensor networks, which makes it possible to better maintain accuracy while compressing the model parameters.
```
```markdown
Simon Fraser University, SFU

Jul. 2020 - Present(Due to the epidemic, the internship became online)

-Advisor: Ivan Bajic
-Focus: Picture Processing; Model Compression
```

## Publications
[1]Maolin Wang*, \mathrm{Zeyong Su*}, Yu Pan, Xu Luo, Shenggen Zheng, and Zenglin Xu. Concatenated Tensor Networks for Deep Multi-Task Learning. Accepted by ICONIP 2020.
[2]\mathrm{Zeyong Su}, Maolin Wang, Yu Pan, and Zenglin Xu. Cumulative-Density based Model for General Temporal Point Processes. Submitted to AAAI 2021.
[3]Yu Pan, \mathrm{Zeyong Su}, Maolin Wang. Unified Initialization For Tensor Neural Network. Submitting to ICLR 2021.

## Awards
1.Canada Mitacs International Undergraduate Summer Research Internship Candidate (Jul. 2020)
2.Research Star Scholarship (Aug. 2019)
3.American University Summer Exchange Program Award (Aug. 2018)
4.Third prize in Mathematical Modeling of University of Electronic Science and Technology of China (Jul. 2020)
5.Outstanding Member of the Student Union Award (Feb. 2018)

## Others
Studied at American University (AU) and worked with collaborators to complete a mock business analysis project (Jul. 2018 - Aug. 2018).
